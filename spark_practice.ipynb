{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb55243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T11:18:04.494571Z",
     "start_time": "2023-05-02T11:18:04.426630Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import geohash2 as gh\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "from pyspark.sql.functions import udf, avg, split, col\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1154625",
   "metadata": {},
   "source": [
    "## Initialization and Definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "646ba3ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T11:40:21.039397Z",
     "start_time": "2023-05-02T11:40:20.878427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Init SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .appName('workshop1')\\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Init OpenCageGeocode\n",
    "geocoder = OpenCageGeocode(os.environ.get('API_KEY'))\n",
    "\n",
    "# Define a UDF function to find coordinates by country and city name\n",
    "@udf(returnType=StringType())\n",
    "def coordinates_udf(city, country):\n",
    "    query = f\"{city}, {country}\"\n",
    "    result = geocoder.geocode(query)\n",
    "    latitude = \"%.3f\" % result[0]['geometry']['lat']\n",
    "    longitude = \"%.3f\" % result[0]['geometry']['lng']\n",
    "    return f\"{latitude},{longitude}\" \n",
    "\n",
    "# Define a UDF function to calculate a geohash by coordinates\n",
    "@udf(returnType=StringType())\n",
    "def geohash_udf(lat, lng):\n",
    "    lat, lng = float(lat), float(lng)\n",
    "    return gh.encode(lat, lng, precision=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8377aedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-19MCNK2.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>workshop1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23e3f24fc10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d4015",
   "metadata": {},
   "source": [
    "## Task 1: \n",
    "### Check restaurant data for incorrect (null) values (latitude and longitude). For incorrect values, map latitude and longitude from the OpenCage Geocoding API in a job via the REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c453ea52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T11:40:35.632947Z",
     "start_time": "2023-05-02T11:40:35.343655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading Restaurant Data from csv files\n",
    "csv_files = [\n",
    "    os.path.join(\"restaurant_csv\", f) \n",
    "    for f in os.listdir(\"restaurant_csv\") \n",
    "    if f.endswith('.csv')]\n",
    "\n",
    "df_res = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(csv_files)\n",
    "\n",
    "# Chek Incorrect Data\n",
    "df_inc = df_res.filter(\"lat is NULL or lng is NULL\")\n",
    "\n",
    "# Finding coordinates of Incorrect Data\n",
    "df_inc = df_inc.withColumn('coordinates', coordinates_udf('city', 'country'))\n",
    "\n",
    "# Updating the latitude and longitude values and deleting the coordinates column\n",
    "df_cor = df_inc.withColumn(\"lat\", split(col(\"coordinates\"), \",\").getItem(0))\\\n",
    "               .withColumn(\"lng\", split(col(\"coordinates\"), \",\").getItem(1))\\\n",
    "               .drop(\"coordinates\")\n",
    "\n",
    "# Union Restaurant and Corrected Data\n",
    "df_res = df_res.filter(\"lat is NOT NULL and lng is NOT NULL\").union(df_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c37faca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Data:  1 rows\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+--------------------+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city| lat| lng|         coordinates|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+--------------------+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|null|null|34.4014089,-79.38...|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+--------------------+\n",
      "\n",
      "Corrected Data:  1 rows\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----------+-----------+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city|       lat|        lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----------+-----------+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|34.4014089|-79.3864339|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----------+-----------+\n",
      "\n",
      "Restaurant Data: 1997 rows\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+--------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|          city|   lat|     lng|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+--------+\n",
      "|197568495625|          10|    The Golden Spoon|                  24784|     US|       Decatur|34.578| -87.021|\n",
      "| 17179869242|          59|         Azalea Cafe|                  10902|     FR|         Paris|48.861|   2.368|\n",
      "|214748364826|          27|     The Corner Cafe|                  92040|     US|    Rapid City|44.080|-103.250|\n",
      "|154618822706|          51|        The Pizzeria|                  41484|     AT|        Vienna|48.213|  16.413|\n",
      "|163208757312|          65|       Chef's Corner|                  96638|     GB|        London|51.495|  -0.191|\n",
      "| 68719476763|          28|    The Spicy Pickle|                  77517|     US|      Grayling|44.657| -84.744|\n",
      "|223338299419|          28|    The Spicy Pickle|                  36937|     US|        Oswego|43.452| -76.532|\n",
      "|240518168650|          75|     Greenhouse Cafe|                  93164|     NL|     Amsterdam|52.370|   4.897|\n",
      "|128849018936|          57|The Yellow Submarine|                   5679|     FR|         Paris|48.872|   2.335|\n",
      "|197568495635|          20|       The Brasserie|                  24784|     US|Jeffersonville|39.616| -83.612|\n",
      "| 68719476768|          33|   The Blue Elephant|                  77517|     IT|         Milan|45.479|   9.146|\n",
      "| 51539607582|          31|           Bistro 42|                   6934|     IT|         Milan|45.444|   9.153|\n",
      "| 94489280554|          43|      The Food House|                  95399|     FR|         Paris|48.867|   2.329|\n",
      "|206158430215|           8|     The Green Olive|                  53370|     US|   Haltom City|32.789| -97.280|\n",
      "|154618822657|           2|        Bella Cucina|                  41484|     US|   Fort Pierce|27.412| -80.391|\n",
      "| 17179869217|          34|     The Tasty Treat|                  10902|     US|     Green Bay|44.476| -88.077|\n",
      "|  8589934633|          42|     The Daily Scoop|                  12630|     FR|         Paris|48.854|   2.343|\n",
      "|240518168596|          21|      The Lazy Daisy|                  93164|     US|    Mendenhall|39.860| -75.646|\n",
      "|171798691906|          67|  Crimson and Clover|                  65939|     NL|     Amsterdam|52.361|   4.894|\n",
      "| 42949673022|          63|          Cafe Paris|                  89646|     GB|        London|51.508|  -0.107|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Checking: ID = 85899345920\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----------+-----------+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city|       lat|        lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----------+-----------+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|34.4014089|-79.3864339|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Incorrect Data:  {df_inc.count()} rows\")\n",
    "df_inc.show()\n",
    "\n",
    "print(f\"Corrected Data:  {df_cor.count()} rows\")\n",
    "df_cor.show()\n",
    "\n",
    "print(f\"Restaurant Data: {df_res.count()} rows\") \n",
    "df_res.show()\n",
    "\n",
    "print(f\"Checking: ID = 85899345920\") \n",
    "df_res.filter(\"id = 85899345920\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06d7ab",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "### Generate a geohash by latitude and longitude using a geohash library like geohash-java. Your geohash should be four characters long and placed in an extra column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85cbf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a geohash by latitude and longitude for Restaurant Data\n",
    "df_res = df_res.withColumn('geohash', geohash_udf('lat', 'lng'))\n",
    "\n",
    "# Save Restaurant Data to parquet file\n",
    "df_res.write.parquet('restaurant.parquet', mode='overwrite')\n",
    "\n",
    "# Read Restaurant Data from parquet file\n",
    "df_res = spark.read.parquet(\"restaurant.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31126928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Data with GeoHash: 1997 rows\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+--------+-------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|          city|   lat|     lng|geohash|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+--------+-------+\n",
      "|197568495625|          10|    The Golden Spoon|                  24784|     US|       Decatur|34.578| -87.021|   dn4h|\n",
      "| 17179869242|          59|         Azalea Cafe|                  10902|     FR|         Paris|48.861|   2.368|   u09t|\n",
      "|214748364826|          27|     The Corner Cafe|                  92040|     US|    Rapid City|44.080|-103.250|   9xyd|\n",
      "|154618822706|          51|        The Pizzeria|                  41484|     AT|        Vienna|48.213|  16.413|   u2ed|\n",
      "|163208757312|          65|       Chef's Corner|                  96638|     GB|        London|51.495|  -0.191|   gcpu|\n",
      "| 68719476763|          28|    The Spicy Pickle|                  77517|     US|      Grayling|44.657| -84.744|   dpgw|\n",
      "|223338299419|          28|    The Spicy Pickle|                  36937|     US|        Oswego|43.452| -76.532|   dr9x|\n",
      "|240518168650|          75|     Greenhouse Cafe|                  93164|     NL|     Amsterdam|52.370|   4.897|   u173|\n",
      "|128849018936|          57|The Yellow Submarine|                   5679|     FR|         Paris|48.872|   2.335|   u09w|\n",
      "|197568495635|          20|       The Brasserie|                  24784|     US|Jeffersonville|39.616| -83.612|   dph9|\n",
      "| 68719476768|          33|   The Blue Elephant|                  77517|     IT|         Milan|45.479|   9.146|   u0nd|\n",
      "| 51539607582|          31|           Bistro 42|                   6934|     IT|         Milan|45.444|   9.153|   u0nd|\n",
      "| 94489280554|          43|      The Food House|                  95399|     FR|         Paris|48.867|   2.329|   u09t|\n",
      "|206158430215|           8|     The Green Olive|                  53370|     US|   Haltom City|32.789| -97.280|   9vff|\n",
      "|154618822657|           2|        Bella Cucina|                  41484|     US|   Fort Pierce|27.412| -80.391|   dhyg|\n",
      "| 17179869217|          34|     The Tasty Treat|                  10902|     US|     Green Bay|44.476| -88.077|   dpcm|\n",
      "|  8589934633|          42|     The Daily Scoop|                  12630|     FR|         Paris|48.854|   2.343|   u09t|\n",
      "|240518168596|          21|      The Lazy Daisy|                  93164|     US|    Mendenhall|39.860| -75.646|   dr44|\n",
      "|171798691906|          67|  Crimson and Clover|                  65939|     NL|     Amsterdam|52.361|   4.894|   u173|\n",
      "| 42949673022|          63|          Cafe Paris|                  89646|     GB|        London|51.508|  -0.107|   gcpv|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Restaurant Data with GeoHash: {df_res.count()} rows\") \n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43080664",
   "metadata": {},
   "source": [
    "## Task 3: \n",
    "### Left-join weather and restaurant data using the four-character geohash. Make sure to avoid data multiplication and keep your job idempotent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774ad874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T11:13:03.162405Z",
     "start_time": "2023-05-02T11:13:01.294219Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading Weather Data from parquet file\n",
    "df_wea = spark.read\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .option(\"recursiveFileLookup\", \"true\")\\\n",
    "    .parquet('weather')\n",
    "\n",
    "# Generate a geohash by latitude and longitude for Weather Data\n",
    "df_wea = df_wea.withColumn('geohash', geohash_udf('lat', 'lng'))\\\n",
    "               \n",
    "# Drop unnecessary columns\n",
    "df_wea = df_wea.drop('lat', 'lng')\n",
    "\n",
    "# Drop all duplicates by wthr_date and geohash in Weather Data\n",
    "df_wea = df_wea.dropDuplicates(['wthr_date', 'geohash'])\n",
    "\n",
    "# Save Weather Data to parquet file\n",
    "df_wea.write.parquet('weather.parquet', mode='overwrite')\n",
    "\n",
    "# Read Weather Data from parquet file\n",
    "df_wea = spark.read.parquet(\"weather.parquet\")\n",
    "\n",
    "# Join Restaurant and Weather Data\n",
    "res_data = df_res.join(df_wea, 'geohash', 'left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00534b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Data:  31882677 rows\n",
      "+----------+----------+----------+-------+\n",
      "|avg_tmpr_f|avg_tmpr_c| wthr_date|geohash|\n",
      "+----------+----------+----------+-------+\n",
      "|      83.6|      28.7|2017-08-29|   d75x|\n",
      "|      75.8|      24.3|2017-08-29|   9et5|\n",
      "|      81.6|      27.6|2017-08-29|   9gf0|\n",
      "|      83.3|      28.5|2017-08-29|   d7t9|\n",
      "|      84.4|      29.1|2017-08-29|   dk6b|\n",
      "|      81.6|      27.6|2017-08-29|   9skp|\n",
      "|      84.4|      29.1|2017-08-29|   9uf2|\n",
      "|      83.6|      28.7|2017-08-29|   9ubc|\n",
      "|      79.2|      26.2|2017-08-29|   9sy9|\n",
      "|      82.3|      27.9|2017-08-29|   9ufs|\n",
      "|      81.8|      27.7|2017-08-29|   9ufj|\n",
      "|      82.8|      28.2|2017-08-29|   9v0c|\n",
      "|      69.1|      20.6|2017-08-29|   9mjc|\n",
      "|      92.1|      33.4|2017-08-29|   9mpq|\n",
      "|      74.0|      23.3|2017-08-29|   9vrk|\n",
      "|      81.7|      27.6|2017-08-29|   9mnq|\n",
      "|      76.7|      24.8|2017-08-29|   9veq|\n",
      "|      73.8|      23.2|2017-08-29|   9vtp|\n",
      "|      70.0|      21.1|2017-08-29|   djb2|\n",
      "|      73.9|      23.3|2017-08-29|   9vuf|\n",
      "+----------+----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Result Data: 172591 rows\n",
      "+-------+------------+------------+----------------+-----------------------+-------+----------+------+-------+----------+----------+----------+\n",
      "|geohash|          id|franchise_id|  franchise_name|restaurant_franchise_id|country|      city|   lat|    lng|avg_tmpr_f|avg_tmpr_c| wthr_date|\n",
      "+-------+------------+------------+----------------+-----------------------+-------+----------+------+-------+----------+----------+----------+\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      70.0|      21.1|2017-08-26|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      64.8|      18.2|2017-09-13|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      72.7|      22.6|2017-09-20|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      65.9|      18.8|2017-09-14|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      71.7|      22.1|2017-08-06|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      74.8|      23.8|2017-08-13|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      60.3|      15.7|2016-10-15|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      59.9|      15.5|2016-10-13|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      58.8|      14.9|2016-10-21|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      60.3|      15.7|2016-10-28|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      64.2|      17.9|2016-10-30|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      65.4|      18.6|2016-10-18|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      66.4|      19.1|2017-08-29|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      58.4|      14.7|2017-09-10|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      60.8|      16.0|2017-09-09|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      77.4|      25.2|2017-08-21|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      57.7|      14.3|2017-09-11|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      71.6|      22.0|2017-09-21|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      64.5|      18.1|2017-09-29|\n",
      "|   dnqx|103079215121|          18|The Harvest Room|                   4340|     US|Mount Airy|36.481|-80.602|      53.7|      12.1|2016-10-11|\n",
      "+-------+------------+------------+----------------+-----------------------+-------+----------+------+-------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weather Data:  {df_wea.count()} rows\")\n",
    "df_wea.show()\n",
    "\n",
    "print(f\"Result Data: {res_data.count()} rows\") \n",
    "res_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9e79e",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "### Store the enriched data (i.e., the joined data with all the fields from both datasets) in the local file system, preserving data partitioning in the parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35deb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Joined data to parquet file\n",
    "res_data.write.partitionBy('country', 'city').parquet('result_data.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa8527",
   "metadata": {},
   "source": [
    "## Task 5:\n",
    "### Implement tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b40d0cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T11:58:22.335823Z",
     "start_time": "2023-05-02T11:58:22.289060Z"
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class TestUDFs(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"test_udfs\")\\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        cls.spark.stop()\n",
    "\n",
    "    def test_coordinates_udf(self):\n",
    "\n",
    "        # Set up test data\n",
    "        columns = [\"country\", \"city\", \"coordinates\"]\n",
    "        data = [[\"USA\", \"New York\",  \"\"],[\"France\", \"Paris\",  \"\"],[\"Australia\", \"Sydney\",  \"\"]]\n",
    "\n",
    "        # Set up expected data\n",
    "        expected_data = [\n",
    "            [\"USA\", \"New York\",  \"40.713,-74.006\"],\n",
    "            [\"France\", \"Paris\",  \"48.853,2.348\"],\n",
    "            [\"Australia\", \"Sydney\",  \"-33.870,151.208\"]]\n",
    "\n",
    "        # Create dataframes\n",
    "        df = self.spark.createDataFrame(data, columns)\n",
    "        expected_df = self.spark.createDataFrame(expected_data, columns)\n",
    "        \n",
    "        # Apply UDF\n",
    "        df = df.withColumn(\"coordinates\", coordinates_udf(\"country\", \"city\"))\n",
    "\n",
    "        # Gather result rows\n",
    "        rows = df.collect()\n",
    "        expected_rows = expected_df.collect()\n",
    "\n",
    "        # Compare dataframes row by row\n",
    "        for row_num, row in enumerate(rows):\n",
    "            self.assertEqual(row, expected_rows[row_num])\n",
    "     \n",
    "    \n",
    "    def test_geohash_udf(self):\n",
    "\n",
    "        # Set up test data\n",
    "        columns = [\"latitude\", \"longitude\", \"geohash\", ]\n",
    "        data = [[40.713, -74.006,  \"\"],[48.853, 2.348,  \"\"],[-33.870, 151.208,  \"\"]]\n",
    "\n",
    "        # Set up expected data\n",
    "        expected_data = [\n",
    "            [40.713, -74.006, \"dr5r\"],\n",
    "            [48.853, 2.348, \"u09t\"],\n",
    "            [-33.870, 151.208, \"r3gx\"]]\n",
    "\n",
    "        # Create dataframes\n",
    "        df = self.spark.createDataFrame(data, columns)\n",
    "        expected_df = self.spark.createDataFrame(expected_data, columns)\n",
    "        \n",
    "        # Apply UDF\n",
    "        df = df.withColumn(\"geohash\", geohash_udf(\"latitude\", \"longitude\"))\n",
    "\n",
    "        # Gather result rows\n",
    "        rows = df.collect()\n",
    "        expected_rows = expected_df.collect()\n",
    "\n",
    "        # Compare dataframes row by row\n",
    "        for row_num, row in enumerate(rows):\n",
    "            self.assertEqual(row, expected_rows[row_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e832eb8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T12:01:26.605449Z",
     "start_time": "2023-05-02T11:58:23.460079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_coordinates_udf (__main__.TestUDFs) ... ok\n",
      "test_geohash_udf (__main__.TestUDFs) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 121.888s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the test class\n",
    "test_suite = unittest.TestLoader().loadTestsFromTestCase(TestUDFs)\n",
    "\n",
    "# Run the tests\n",
    "unittest.TextTestRunner(verbosity=2).run(test_suite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d8fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
